---
layout: post
title: Tornado Peril
landing-title: 'Blog'
description: A rapid, sudden shift...
image: assets/images/pic12.png
show_tile: true
nav-menu: true
---

In this blog post, we'll be discussing our approach to the tornado peril and how it is applied in the application of data engineering and business.

We use geographic data to give us high granular actuarial statistics to quanitfy financial risk across an portfolio of assets.

For the example, we'll be looking at a portfolio of real estate assets.



![image info](/assets/images/pic12.jpg)



<h2>Tornado Statistics for Underwriting</h2>
<dl>
	<dt>`avg_frequency`</dt>
	<dd>
		<p>
        The avg_frequency is a measure of how often tornado events occur in a particular hexagon, on average. It is calculated by dividing the total number of tornado events in a hexagon by the sum of the years difference between each event.
		For example, suppose we have a hexagon with 5 tornado events, and the years difference between each event is 1, 2, 3, 4, and 5 years, respectively. The sum of the years difference is 1+2+3+4+5=15. So the average frequency of tornado events in this hexagon would be 5/15=0.33 events per year, on average.</p>
	</dd>
	<dt>Expected Loss</dt>
	<dd>
		<p>def calculate_expected_loss(insured_values, tornado_probabilities):
    expected_losses = [value * probability for value, probability in zip(insured_values, tornado_probabilities)]
        return expected_losses.</p>
	</dd>
</dl>

#### **Workflow: Process**

1. Ingest Tornado data public **REST API** service and stage in **PostgreSQL**
   1. Query API; paginate
   2. Store as GeoDataFrame
   3. Write to PostGIS
   4. Buffer geometry by 0.25 mile
2. Stage **Uber H3** hexagon layers in PostgreSQL by US state
     1. Loop through each state and create individual table name `{us_state}_h3`
3. Run an spatial intersection using **PostGIS** functions
      1. Loop through each state and perform spatial join between tornado buffer geometry and H3.
4. Dissolve by the `h3_id` to aggregate statistics into each H3 cell
5. **Summarize statistics** in a list array
6.  Join each state into one layer
7.  Write to PostgresSQL

We use Uber H3 hexagons to aggregate our geographic data together.  We use H3 because it:
-  allows a standard geometry (that is immutable, or never changes its shape) to aggregate data to
-  because it makes geography a controlled variable in ML/AI pipelines.  
  
This allows data and computer vision scientists to easily assess geographic and non-geographic variables when fitting and explaining predictive analytics.


### **Tornado Risk of Arkansas Real Estate properties**

![image info](/assets/images/arkansas_risk_realestate.png)



1. The map above shows real estate most at-risk to tornados, shown in yellow.
2. Real Estate bought or owned, will incur loss given it's geographic setting
3. Below, we apply  high resolution annual probabilities , inputting these into common equations performed by actuaries and underwriters.



![image info](/assets/images/arkansas_risk_stats.png)

#### Using `annual_probabilities`, we can look up each individual property and use an more precise input for calculating the `annual_probability` which is the base for an `expected_annual_loss` equation.

#### `annual_probabilities` data comes in a resolution of 110m^2.  Meaning, we space our **Uber H3 hexagons** to resolution 9, or ~110m^2 area indexed and assessed across the conterminous US.