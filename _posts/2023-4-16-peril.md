---
layout: post
title: Tornado Peril
landing-title: 'Blog'
description: A rapid, sudden shift...
image: assets/images/pic12.png
show_tile: true
nav-menu: true
---

In this blog post, we'll be discussing our approach to the tornado peril and how it is applied in the application of data engineering and business.

We use geographic data to give us high granular actuarial statistics to quanitfy financial risk across an portfolio of assets.

For the example, we'll be looking at a portfolio of real estate assets.



![image info](/assets/images/pic12.jpg)



<h2>Tornado Statistics for Underwriting</h2>
<dl>
	<dt>`annual_probabilities`</dt>
	<dd>
		<p>
        The `annual_probabilities` is a measure of how often tornado events occur in a particular hexagon, on average. It is calculated by dividing the total number of tornado events in a hexagon by the sum of the years difference between each event.
		For example, suppose we have a hexagon with 5 tornado events, and the years difference between each event is 1, 2, 3, 4, and 5 years, respectively. The sum of the years difference is 1+2+3+4+5=15. So the average frequency of tornado events in this hexagon would be 5/15=0.33 events per year, on average.</p>
	</dd>
	<dt>Expected Loss</dt>
	<dd>
		<p>The Annual Expected Loss (AEL) equation in the insurance industry is a statistical measure used to estimate the amount of money an insurer is likely to pay out on claims over the course of a year. It is calculated by multiplying the Probability of Loss (PoL) by the Expected Loss Amount (ELA).

Mathematically, the equation for Annual Expected Loss is:

AEL = PoL x ELA

where:

PoL is the probability of a loss occurring within a given period of time (usually one year)
ELA is the expected loss amount for a single loss event
To calculate the Annual Expected Loss, an insurer will typically use historical data to estimate the PoL and ELA for a particular type of insurance policy. The insurer may also factor in other variables, such as the level of risk associated with a particular policy, to refine their estimate of the AEL. By estimating the AEL, insurers can better understand the potential financial impact of insuring a particular risk and set appropriate premium rates to cover their expected losses.</p>
	</dd>
</dl>

### **Tornado Risk of Arkansas Real Estate properties**

![image info](/assets/images/arkansas_risk_realestate.png)



1. The map above shows real estate most at-risk to tornados, shown in yellow.
2. Real Estate bought or owned, will incur loss given it's geographic setting
3. Below, we apply  high resolution annual probabilities , inputting these into common equations performed by actuaries and underwriters.



![image info](/assets/images/arkansas_risk_stats.png)

#### Using `annual_probabilities`, we can look up each individual property and use an more precise input for calculating the `annual_probability` which is the base for an `expected_annual_loss` equation.

#### `annual_probabilities` data comes in a resolution of 110m^2.  Meaning, we space our **Uber H3 hexagons** to resolution 9, or ~110m^2 area indexed and assessed across the conterminous US.


#### **Workflow: Process**

1. Ingest Tornado data public **REST API** service and stage in **PostgreSQL**
   1. Query API; paginate
   2. Store as GeoDataFrame
   3. Write to PostGIS
   4. Buffer geometry by 0.25 mile
2. Stage **Uber H3** hexagon layers in PostgreSQL by US state
     1. Loop through each state and create individual table name `{us_state}_h3`
3. Run an spatial intersection using **PostGIS** functions
      1. Loop through each state and perform spatial join between tornado buffer geometry and H3.
4. Dissolve by the `h3_id` to aggregate statistics into each H3 cell
5. **Summarize statistics** in a list array
6.  Join each state into one layer
7.  Write to PostgresSQL

We use Uber H3 hexagons to aggregate our geographic data together.  We use H3 because it:
-  allows a standard geometry (that is immutable, or never changes its shape) to aggregate data to
-  because it makes geography a controlled variable in ML/AI pipelines.  
  
This allows data and computer vision scientists to easily assess geographic and non-geographic variables when fitting and explaining predictive analytics.
