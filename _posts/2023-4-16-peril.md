---
layout: post
title: Tornado Peril
landing-title: 'Blog'
description: A rapid, sudden shift...
image: assets/images/pic12.png
show_tile: true
nav-menu: true
---

*Black Dots - Buildings / Real Estate*

In this blog post, we'll be discussing our approach to the tornado peril and how that changes our approach.

We rely on geographic data to make this happen and the end result gives us actuarial statistics to quanitfy risk to an portfolio of assets.

For the example, we'll be looking at a portfolio of real estate assets.



![image info](/assets/images/pic12.jpg)



<h2>Tornado Statistics for Underwriting</h2>
<dl>
	<dt>`avg_frequency`</dt>
	<dd>
		<p>
        The avg_frequency is a measure of how often tornado events occur in a particular hexagon, on average. It is calculated by dividing the total number of tornado events in a hexagon by the sum of the years difference between each event.
		For example, suppose we have a hexagon with 5 tornado events, and the years difference between each event is 1, 2, 3, 4, and 5 years, respectively. The sum of the years difference is 1+2+3+4+5=15. So the average frequency of tornado events in this hexagon would be 5/15=0.33 events per year, on average.</p>
	</dd>
	<dt>Expected Loss</dt>
	<dd>
		<p>def calculate_expected_loss(insured_values, tornado_probabilities):
    expected_losses = [value * probability for value, probability in zip(insured_values, tornado_probabilities)]
        return expected_losses.</p>
	</dd>
</dl>

#### **Workflow: Process**

1. Ingest Tornado data public **REST API** service and stage in **PostgreSQL**
   1. Query API; paginate
   2. Store as GeoDataFrame
   3. Write to PostGIS
   4. Buffer geometry by 0.25 mile
2. Stage **Uber H3** hexagon layers in PostgreSQL by US state
     1. Loop through each state and create individual table name `{us_state}_h3`
3. Run an spatial intersection using **PostGIS** functions
      1. Loop through each state and perform spatial join between tornado buffer geometry and H3.
4. Dissolve by the `h3_id` to aggregate statistics into each H3 cell
5. **Summarize statistics** in a list array
6.  Join each state into one layer
7.  Write to PostgresSQL

We use Uber H3 hexagons to aggregate our geographic data together.  We use H3 because it:
-  allows a standard geometry (that is immutable, or never changes its shape) to aggregate data to
-  because it makes geography a controlled variable in ML/AI pipelines.  
  
This allows data and computer vision scientists to easily assess geographic and non-geographic variables when fitting and explaining predictive analytics.




![image info](/assets/images/arkansas_risk_realestate.png)

#### **Tornado Risk of Arkansas Real Estate properties**

1. The map above shows real estate most at-risk to tornados, shown in yellow.
2. Real Estate bought or owned, will incur loss given it's geographic setting
3. Below, we'll look at using the high resolution annual probabilities we produce, we can input these into common equations performed by actuaries and underwriters.



![image info](/assets/images/arkansas_risk_stats.png)

#### Using `avg_frequency` from the Alsace Research API, we can look up each individual property and use an more precise input for calculating the `annual_probability` which is the base for an `expected_annual_loss` equation.